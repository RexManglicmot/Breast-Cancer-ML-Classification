---
title: "Breast Cancer ML Classifcation Linear Discriminant Analysis"
author: "Rex Manglicmot"
output: 
 github_document: 
    toc: yes
 
---

## Status: Continuing Working Document
Hi everyone. I’m continuing building my data analysis and R skills. As such, I would love feedback to better improve this project via rexmanglicmot@gmail.com. Any mistakes and misrepresentation of the data are my own.

Things Need To Do/Questions:

* More concepts about LDA + include mathematical expressions and the verbal walkthrough of such.
* Grammar.
* Get feedback and incorporate
* Fill in intro
* How do I create a boxplot from all the quant columns against a column that has 2 categorical variables? I know how to do this within a single column, but not multiple. Ditto for bar charts?




## Introduction

![](https://health.osu.edu/-/media/health/images/stories/2022/09/breast-scan.jpg?la=en&hash=F19C705F3A4E59F446666BF1EB7079DB)

There are many factors that attribute to formation cancer, but there is one dominant factor among all cancers, and that is gene expression. There are numerous studies that have shown that having a certain gene may likely contribute to a formation of a certain cancer. The human genome is estimated to have about 20,000 genes.^[https://sitn.hms.harvard.edu/flash/2012/issue127a/] These genes then go through mechanisms of protein synthesis which in the end create a vast array of proteins (estimated to be 20,000 or even more^[https://theconversation.com/what-is-a-protein-a-biologist-explains-152870#:~:text=Scientists%20are%20not%20exactly%20sure,causing%20your%20muscles%20to%20work.]) and these proteins  then have the capability to form cells (and eventually cancer).

Studying genes and their expressions provide scientist the opportunity to idneitfy which patients are susceptible to certain cancers.

The project is structured in the following chapters:

1. Introduction
2. Loading the Libraries
3. Loading the Data
4. Cleaning the Data
5. Exploratory Data Analysis
6. Modeling: Linear Discriminant Analysis
7. Limitations
8. Conclusion
9. Inspiration for this project

A special acknowledgement to the University of Irvine Data Repository for providing the dataset to the public.^[https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Coimbra] A special acknowledgement for Miguel Patricio et al. from the Faculty of Medicine of the University of Coimbra and also Manuel Gomes from the University Hospital Centre of Coimbra for their research and for providing this dataset.

Within the dataset there are 10 variables:

1. Age (years)
2. BMI (kg/m2)
3. Glucose (mg/dL)
4. Insulin (µU/mL)
5. HOMA
6. Leptin (ng/mL)
7. Adiponectin (µg/mL)
8. Resistin (ng/mL)
9. MCP-1(pg/dL)
10. Classification: 1-Healthy Controls and 2-BC Patients



## Loading the Libraries
```{r, message = FALSE}
#install.packages('data.table') #installed on 12/14/22
#install.packages('ggExtra')

#Load the libraries
library(tidyverse)
library(dplyr)
library(data.table)
library(corrplot)
library(ggExtra)
library(ggplot2)
```

## Loading the Data
```{r}
#load the dataset
data <- read.csv('dataR2.csv')

#view the first few rows of the data
head(data, 10)

#look at the number of the rows and columns we have
dim(data)
```
The dataset has the 10 variables and 116 observations. So far we were able to download the data correctly.

## Cleaning the Data
Let's create a new object of the orginal data and change the variable names. Some of them like Classification are too long and let's change them to a name much shorter.
```{r}
#store data into a new object for data cleaning
data2 <- data

#change variable name to lowercase
names(data2) <- tolower(names(data2))

#change variable name to 3-4 words
names(data2)[3:10] <- c('glu', 'ins', 'hom', 'lep', 'adi', 'res', 'mcp', 'type')
```

Let's inspect our data for any NA values and see all the variables are correctly classed.
```{r}
#check for any NA values
sum(is.na(data2))

#another method to check for NA values
any(is.na(data2))

#check to see if there are any alphabet letters in the dataframe since we are dealing with a quant dataset
sapply(sapply(data2, grep, pattern = "[a-z]+"), length)

#check class other than str function
data.frame(sapply(data2,class))
```
Everything looks ok except for the type variable; it needs to be a factor.

```{r}
#change last column to a factor
data2$type=as.factor(data2$type)

#check if it was changed
class(data2$type)
```
Now, let's explore!



## Exploratory Data Analysis
```{r}
summary(data2)
```


```{r}
# ggplot(data2, aes(x=type, color =(.))) +
#   geom_boxplot()
```


```{r}
#create barcharts
ggplot(data2, aes(type, fill= type)) +
  geom_bar(width = .4, color='black') +
  theme_linedraw() +
 scale_fill_brewer(palette="Reds") +
  labs(title = 'Number of Control and Breast Cancer Patients',
       x = 'Participants',
       caption = '1 = Control Group and 2 = Breast Cancer Group',
       y = 'Count') +
  theme(plot.title = element_text(hjust = 0.5),
        plot.caption = element_text(hjust = 0.5),
        axis.title = element_text(face = "bold"))
```

```{r}
# 
# data3 <- data2 %>%
#   group_by(type) %>%
#   summarise(across(everything(), mean),
#             .groups = 'drop') %>%
#   as.data.frame()
# 
# names(data4)
#   
# data4 <- data4[-1, ] %>%
#   rename('1' = c1,
#          '2' = c2)
# 
# ggplot(data4, aes(x=1)) +
#   geom_bar()
```

```{r}
# ggplot(data4, aes(x=V1, y=V2)) +
#   geom_bar()
```


```{r, warning= FALSE}
g <- ggplot(data2, aes(x=type, y=age)) +
  geom_point(position = 'jitter', color = 'black') +
  theme_gray() +
   labs(title = 'Type against Age + Overall Age Distribution',
       x = 'Participants',
       caption = '1 = Control Group and 2 = Breast Cancer Group',
       y = 'Age Range') +
  theme(plot.title = element_text(hjust = 0.5, size=10),
        plot.caption = element_text(hjust = 0.5))

ggMarginal(g, type = 'histogram',
           margins = 'y',
           size = 3,
           fill=2)
```


```{r}
g2 <- ggplot(data2, aes(x=type, y=bmi)) +
  geom_point(position = 'jitter') +
  theme_gray() +
   labs(title = 'Type against BMI + Overall BMI Distribution',
       x = 'Participants',
       caption = '1 = Control Group and 2 = Breast Cancer Group',
       y = 'BMI Range') +
  theme(plot.title = element_text(hjust = 0.5, size=10),
        plot.caption = element_text(hjust = 0.5))

ggMarginal(g2, type = 'histogram',
           margins = 'y',
           size = 3,
           fill=7)
```


```{r}
g3 <- ggplot(data2, aes(x=type, y=glu)) +
  geom_point(position = 'jitter', color = 'black') +
  theme_gray() +
   labs(title = 'Type against Glucose + Overall Glucose Distribution',
       x = 'Participants',
       caption = '1 = Control Group and 2 = Breast Cancer Group',
       y = 'Glucose Range') +
  theme(plot.title = element_text(hjust = 0.5, size=10),
        plot.caption = element_text(hjust = 0.5))

ggMarginal(g3, type = 'density',
           margins = 'y',
           size = 3,
           fill=13)
```


```{r}
g4 <- ggplot(data2, aes(x=type, y=ins)) +
  geom_point(position = 'jitter', color = 'black') +
  theme_gray() +
   labs(title = 'Type against Insulin + Overall Insulin Distribution',
       x = 'Participants',
       caption = '1 = Control Group and 2 = Breast Cancer Group',
       y = 'Insulin Range') +
  theme(plot.title = element_text(hjust = 0.5, size=10),
        plot.caption = element_text(hjust = 0.5))

ggMarginal(g4, type = 'density',
           margins = 'y',
           size = 3,
           fill=3)
```



## Modeling: Linear Discriminant Analysis
![](https://repository-images.githubusercontent.com/13402862/b8473680-4379-11ea-8d72-e87a518def4b)

**Linear Discriminant Analysis** is a dimensional reduction technique used to separate two or more classes.^[https://www.geeksforgeeks.org/ml-linear-discriminant-analysis/#:~:text=Linear%20Discriminant%20Analysis%20or%20Normal,separating%20two%20or%20more%20classes.] The goal is to project the data onto a *lower-dimensional space* that has decent class separability.^[https://sebastianraschka.com/Articles/2014_python_lda.html]

LDA is a type of dimensional reduction technique that reduces the number of dimensions in the dataset.^[https://www.mygreatlearning.com/blog/linear-discriminant-analysis-or-lda/] The objective is to reduce a high dimensional dataset to a lower dimensional space and to have separation between classes (therefore, our classification)^[https://www.digitalvidya.com/blog/linear-discriminant-analysis/] Recall that Multidimensional data has many features (i.e. variables) that be are correlated to one another and dimensional reduction reduces this to 2 or 3 dimensions.^[https://www.digitalvidya.com/blog/linear-discriminant-analysis/]

I decided to pput this in another object for closer inspection and saw that 267 variables out of the 20531 variables had zero values. Now let's get rid of this of these columns and create a new object, data4, that contains all non-zero, numeric values.

Below are the Steps to do an LDA^[https://www.mygreatlearning.com/blog/linear-discriminant-analysis-or-lda/]:

1. Compute Mean vectors of each class of dependent variables

2. Compute with-in class and between-class scatter matrices

3. Compute Eigenvalues and Eigenvector for SW and SB

4. Sort the Eigenvalues in discending order and select the top k

5. Create a new matrix containing eignevectors that map the k eigenvalues

6. Contain the new features (linear discriminants) by taking the dot plot of the data and the matrix. 

![](https://media.geeksforgeeks.org/wp-content/uploads/20190423132508/1dlda.jpg)
```{r}

```



## Limitations


## Conclusion


## Appendix
```{r, warning=FALSE, message= FALSE}
library(car)
```


```{r, message= FALSE}
#curious to see the relation age and bmi
ggplot(data2, aes(age, bmi)) +
  geom_point() +
   geom_smooth(method=lm)

scatterplot(bmi ~ age, data=data2)
#this plot contains:
#the points
#the regression line (in blue)
#the smoothed conditional spread (in blue dashed line)
#the non-parametric regression smooth (light blue)
```

In both graphs, it seems there is no relationship of age and BMI.

```{r, message= FALSE}
#curious to see the relation age and bmi
ggplot(data2, aes(age, glu)) +
  geom_point() +
   geom_smooth(method=lm)

scatterplot(glu ~ age, data=data2, col='orange')
```

It would seem the slope is positive and there could be a relationship. 

```{r, message= FALSE}
#curious to see the relation age and bmi
ggplot(data2, aes(age, ins)) +
  geom_point() +
   geom_smooth(method=lm)

scatterplot(ins ~ age, data=data2, col='magenta')
```

Slope looks flat.


## Inspiration for this project